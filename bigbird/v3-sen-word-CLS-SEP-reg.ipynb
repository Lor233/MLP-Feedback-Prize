{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8523a158",
   "metadata": {
    "papermill": {
     "duration": 0.021776,
     "end_time": "2021-12-24T18:56:32.200543",
     "exception": false,
     "start_time": "2021-12-24T18:56:32.178767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PyTorch BigBird NER Baseline - CV 0.615\n",
    "This notebook is a PyTorch starter notebook for Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. It demonstrates how to train, infer, and submit a model to Kaggle without internet. Currently this notebook uses\n",
    "\n",
    "* backbone BigBird  (with HuggingFace's head for TokenClassification)\n",
    "* NER formulation (with `is_split_into_words=True` tokenization)\n",
    "* one fold\n",
    "\n",
    "By changing a few lines of code, we can use this notebook to evaluate different PyTorch backbones! And we can run all sorts of other experiments. If we try a backbone that doesn't accept 1024 wide tokens (like BigBird or LongFormer), then we can add a sliding window to train and inference. BigBird is a new SOTA transformer with arXiv paper [here][3] which can accept large token inputs as wide as 4096!\n",
    "\n",
    "The model in this notebook uses HuggingFace's `AutoModelForTokenClassification`. If we want a custom head, we could use `AutoModel` and then build our own head. See my TensorFlow notebook [here][2] for an example.\n",
    "\n",
    "The tokenization process uses `tokenizer(txt.split(), is_split_into_words=True)`, note that this ignores characters like `\\n`. If we want our model to see new paragraphs, we need to rewrite this code and avoid `is_split_into_words=True`. See my TensorFlow notebook [here][2] for an example.\n",
    "\n",
    "This notebook uses many code cells from Raghavendrakotala's great notebook [here][1]. Don't forget to upvote Raghavendrakotala's notebook :-)\n",
    "\n",
    "[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "[2]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n",
    "[3]: https://arxiv.org/abs/2007.14062"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f697b1",
   "metadata": {
    "papermill": {
     "duration": 0.020383,
     "end_time": "2021-12-24T18:56:32.243330",
     "exception": false,
     "start_time": "2021-12-24T18:56:32.222947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration\n",
    "This notebook can either train a new model or load a previously trained model (made from previous notebook version). Furthermore, this notebook can either create new NER labels or load existing NER labels (made from previous notebook version). In this notebook version, we will load model and load NER labels.\n",
    "\n",
    "Also this notebook can load huggingface stuff (like tokenizers) from a Kaggle dataset, or download it from internet. (If it downloads from internet, you can then put it in a Kaggle dataset, so next time you can turn internet off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67498c5a-c986-4c6c-8ed2-054a2ad8049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a2a146dd",
   "metadata": {
    "papermill": {
     "duration": 0.039536,
     "end_time": "2021-12-24T18:56:32.303289",
     "exception": false,
     "start_time": "2021-12-24T18:56:32.263753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# DECLARE HOW MANY GPUS YOU WISH TO USE. \n",
    "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n",
    "\n",
    "# VERSION FOR SAVING MODEL WEIGHTS\n",
    "TRAIN_VER = 1\n",
    "LOAD_VER = 26\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n",
    "# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n",
    "LOAD_TOKENS_FROM = 'NER_token'\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n",
    "# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n",
    "# LOAD_MODEL_FROM = 'trained_model'\n",
    "LOAD_MODEL_FROM = None\n",
    "\n",
    "# IF FOLLOWING IS NONE, THEN NOTEBOOK \n",
    "# USES INTERNET AND DOWNLOADS HUGGINGFACE \n",
    "# CONFIG, TOKENIZER, AND MODEL\n",
    "# DOWNLOADED_MODEL_PATH = '../input/py-bigbird-v26' \n",
    "DOWNLOADED_MODEL_PATH = None\n",
    "\n",
    "if DOWNLOADED_MODEL_PATH is None:\n",
    "    DOWNLOADED_MODEL_PATH = 'model'    \n",
    "MODEL_NAME = 'google/bigbird-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "844706b2",
   "metadata": {
    "papermill": {
     "duration": 1.773177,
     "end_time": "2021-12-24T18:56:34.097096",
     "exception": false,
     "start_time": "2021-12-24T18:56:32.323919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "import torch\n",
    "\n",
    "config = {'model_name': MODEL_NAME,\n",
    "         'max_length': 1024,\n",
    "         # 'train_batch_size':4,\n",
    "         # 'valid_batch_size':4,\n",
    "         'train_batch_size':1,\n",
    "         'valid_batch_size':3,\n",
    "         'epochs':5,\n",
    "         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "         'max_grad_norm':10,\n",
    "         'device':  'cuda' if cuda.is_available() else 'cpu'\n",
    "         # 'device':  'cpu'\n",
    "          }\n",
    "\n",
    "# THIS WILL COMPUTE VAL SCORE DURING COMMIT BUT NOT DURING SUBMIT\n",
    "COMPUTE_VAL_SCORE = True\n",
    "if len( os.listdir('../data/test') ) > 5:\n",
    "      COMPUTE_VAL_SCORE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636dadb",
   "metadata": {
    "papermill": {
     "duration": 0.040966,
     "end_time": "2021-12-24T18:56:34.171732",
     "exception": false,
     "start_time": "2021-12-24T18:56:34.130766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How To Submit PyTorch Without Internet\n",
    "Many people ask me, how do I submit PyTorch models without internet? With HuggingFace Transformer, it's easy. Just download the following 3 things (1) model weights, (2) tokenizer files, (3) config file, and upload them to a Kaggle dataset. Below shows code how to get the files from HuggingFace for Google's BigBird-base. But this same code can download any transformer, like for example roberta-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b55dc175",
   "metadata": {
    "papermill": {
     "duration": 8.963092,
     "end_time": "2021-12-24T18:56:43.170108",
     "exception": false,
     "start_time": "2021-12-24T18:56:34.207016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "if DOWNLOADED_MODEL_PATH == 'model' and not os.path.exists('model'):\n",
    "    os.mkdir('model')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config_model = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    config_model.num_labels = 15\n",
    "    config_model.save_pretrained('model')\n",
    "\n",
    "    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME,\n",
    "                                                               config=config_model)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ae13a",
   "metadata": {
    "papermill": {
     "duration": 0.020416,
     "end_time": "2021-12-24T18:56:43.211378",
     "exception": false,
     "start_time": "2021-12-24T18:56:43.190962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data and Libraries\n",
    "In addition to loading the train dataframe, we will load all the train and text files and save them in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "645e0f5e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.027699,
     "end_time": "2021-12-24T18:56:43.259877",
     "exception": false,
     "start_time": "2021-12-24T18:56:43.232178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np, os \n",
    "import pandas as pd, gc \n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d8bd1d5",
   "metadata": {
    "papermill": {
     "duration": 1.863642,
     "end_time": "2021-12-24T18:56:45.144220",
     "exception": false,
     "start_time": "2021-12-24T18:56:43.280578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144293, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": "             id  discourse_id  discourse_start  discourse_end  \\\n0  423A1CA112E2  1.622628e+12              8.0          229.0   \n1  423A1CA112E2  1.622628e+12            230.0          312.0   \n2  423A1CA112E2  1.622628e+12            313.0          401.0   \n3  423A1CA112E2  1.622628e+12            402.0          758.0   \n4  423A1CA112E2  1.622628e+12            759.0          886.0   \n\n                                      discourse_text discourse_type  \\\n0  Modern humans today are always on their phone....           Lead   \n1  They are some really bad consequences when stu...       Position   \n2  Some certain areas in the United States ban ph...       Evidence   \n3  When people have phones, they know about certa...       Evidence   \n4  Driving is one of the way how to get around. P...          Claim   \n\n  discourse_type_num                                   predictionstring  \n0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_id</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_type_num</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>8.0</td>\n      <td>229.0</td>\n      <td>Modern humans today are always on their phone....</td>\n      <td>Lead</td>\n      <td>Lead 1</td>\n      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>230.0</td>\n      <td>312.0</td>\n      <td>They are some really bad consequences when stu...</td>\n      <td>Position</td>\n      <td>Position 1</td>\n      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>313.0</td>\n      <td>401.0</td>\n      <td>Some certain areas in the United States ban ph...</td>\n      <td>Evidence</td>\n      <td>Evidence 1</td>\n      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>402.0</td>\n      <td>758.0</td>\n      <td>When people have phones, they know about certa...</td>\n      <td>Evidence</td>\n      <td>Evidence 2</td>\n      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>759.0</td>\n      <td>886.0</td>\n      <td>Driving is one of the way how to get around. P...</td>\n      <td>Claim</td>\n      <td>Claim 1</td>\n      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "print( train_df.shape )\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c30751f",
   "metadata": {
    "papermill": {
     "duration": 0.063091,
     "end_time": "2021-12-24T18:56:45.230167",
     "exception": false,
     "start_time": "2021-12-24T18:56:45.167076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             id                                               text\n0  0FB0700DAF44  During a group project, have you ever asked a ...\n1  18409261F5C2  80% of Americans believe seeking multiple opin...\n2  D46BCB48440A  When people ask for advice,they sometimes talk...\n3  D72CB1C11673  Making choices in life can be very difficult. ...\n4  DF920E0A7337  Have you ever asked more than one person for h...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0FB0700DAF44</td>\n      <td>During a group project, have you ever asked a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18409261F5C2</td>\n      <td>80% of Americans believe seeking multiple opin...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>D46BCB48440A</td>\n      <td>When people ask for advice,they sometimes talk...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D72CB1C11673</td>\n      <td>Making choices in life can be very difficult. ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DF920E0A7337</td>\n      <td>Have you ever asked more than one person for h...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "test_names, test_texts = [], []\n",
    "for f in list(os.listdir('../data/test')):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../data/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "test_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3067e7e2",
   "metadata": {
    "papermill": {
     "duration": 58.996899,
     "end_time": "2021-12-24T18:57:44.249132",
     "exception": false,
     "start_time": "2021-12-24T18:56:45.252233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [00:01<00:00, 12302.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "             id                                               text\n0  0000D23A521A  Some people belive that the so called \"face\" o...\n1  00066EA9880D  Driverless cars are exaclty what you would exp...\n2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...\n3  001552828BD0  Would you be able to give your car up? Having ...\n4  0016926B079C  I think that students would benefit from learn...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000D23A521A</td>\n      <td>Some people belive that the so called \"face\" o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00066EA9880D</td>\n      <td>Driverless cars are exaclty what you would exp...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000E6DE9E817</td>\n      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001552828BD0</td>\n      <td>Would you be able to give your car up? Having ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016926B079C</td>\n      <td>I think that students would benefit from learn...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "test_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('../data/train'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open('../data/train/' + f, 'r').read())\n",
    "train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n",
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cfcef8",
   "metadata": {
    "papermill": {
     "duration": 0.168711,
     "end_time": "2021-12-24T18:57:44.586488",
     "exception": false,
     "start_time": "2021-12-24T18:57:44.417777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert Train Text to NER Labels\n",
    "We will now convert all text words into NER labels and save in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c207d26e",
   "metadata": {
    "papermill": {
     "duration": 14.170019,
     "end_time": "2021-12-24T18:57:58.923453",
     "exception": false,
     "start_time": "2021-12-24T18:57:44.753434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15594, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": "             id                                               text  \\\n0  E112CE5938F4  Segoing Cowboys is a good program that I think...   \n1  13326EED1708  Yes I agree that Venus is dangerous place, but...   \n2  DD3F7252F887  Facial action coding sytsem should be used bec...   \n3  E676D0FB128A  My fellow citizens, there are many reasons why...   \n4  9A03E46C0A51  Dear Principal,\\n\\nI believe that changing the...   \n\n                                            entities  \n0  [B-Position, I-Position, I-Position, I-Positio...  \n1  [B-Position, I-Position, I-Position, I-Positio...  \n2  [B-Position, I-Position, I-Position, I-Positio...  \n3  [O, O, O, B-Position, I-Position, I-Position, ...  \n4  [O, O, B-Position, I-Position, I-Position, I-P...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>entities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E112CE5938F4</td>\n      <td>Segoing Cowboys is a good program that I think...</td>\n      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13326EED1708</td>\n      <td>Yes I agree that Venus is dangerous place, but...</td>\n      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DD3F7252F887</td>\n      <td>Facial action coding sytsem should be used bec...</td>\n      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>E676D0FB128A</td>\n      <td>My fellow citizens, there are many reasons why...</td>\n      <td>[O, O, O, B-Position, I-Position, I-Position, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9A03E46C0A51</td>\n      <td>Dear Principal,\\n\\nI believe that changing the...</td>\n      <td>[O, O, B-Position, I-Position, I-Position, I-P...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f'{LOAD_TOKENS_FROM}/train_NER.csv'):\n",
    "    all_entities = []\n",
    "    for ii, i in enumerate(train_text_df.iterrows()):\n",
    "        if ii%100==0: print(ii, ', ' ,end='')\n",
    "        total = i[1]['text'].split().__len__()\n",
    "        entities = [\"O\"] * total\n",
    "        # print(total)\n",
    "        for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n",
    "            discourse = j[1]['discourse_type']\n",
    "            list_ix = [int(x) for x in j[1]['predictionstring'].split(' ')]\n",
    "            entities[list_ix[0]] = f\"B-{discourse}\"\n",
    "            for k in list_ix[1:]: \n",
    "                # entities[k] = f\"I-{discourse}\"\n",
    "                try:\n",
    "                    entities[k] = f\"I-{discourse}\"\n",
    "                except:\n",
    "                    print(len(entities))\n",
    "                    print(k)\n",
    "        all_entities.append(entities)\n",
    "    train_text_df['entities'] = all_entities\n",
    "    train_text_df.to_csv('train_NER.csv',index=False)\n",
    "    \n",
    "else:\n",
    "    from ast import literal_eval\n",
    "    train_text_df = pd.read_csv(f'{LOAD_TOKENS_FROM}/train_NER.csv')\n",
    "    # pandas saves lists as string, we must convert back\n",
    "    train_text_df.entities = train_text_df.entities.apply(lambda x: literal_eval(x) )\n",
    "    \n",
    "print( train_text_df.shape )\n",
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c27e0b80",
   "metadata": {
    "papermill": {
     "duration": 0.176747,
     "end_time": "2021-12-24T18:57:59.269256",
     "exception": false,
     "start_time": "2021-12-24T18:57:59.092509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\n",
    "output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6bd2d164",
   "metadata": {
    "papermill": {
     "duration": 0.176108,
     "end_time": "2021-12-24T18:57:59.613066",
     "exception": false,
     "start_time": "2021-12-24T18:57:59.436958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'O': 0,\n 'B-Lead': 1,\n 'I-Lead': 2,\n 'B-Position': 3,\n 'I-Position': 4,\n 'B-Claim': 5,\n 'I-Claim': 6,\n 'B-Counterclaim': 7,\n 'I-Counterclaim': 8,\n 'B-Rebuttal': 9,\n 'I-Rebuttal': 10,\n 'B-Evidence': 11,\n 'I-Evidence': 12,\n 'B-Concluding Statement': 13,\n 'I-Concluding Statement': 14}"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66a066ef-33be-4bc7-9cf5-da42db7611e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = train_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95ea021a-c784-43ec-b7d2-b2d18500bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_text = '[CLS] ' + text.replace('.', '. [SEP]')\n",
    "# encoding = tokenizer(pre_text.split(),\n",
    "#                     is_split_into_words=True,\n",
    "#                     add_special_tokens=False,\n",
    "#                     return_offsets_mapping=True, \n",
    "#                     padding='max_length', \n",
    "#                     truncation=True, \n",
    "#                     max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7b2eb6b-da26-48a1-907c-11b347341b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoding.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2f416c69-9dc2-4722-b469-8addd0b95aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoding['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f38ce",
   "metadata": {
    "papermill": {
     "duration": 0.166097,
     "end_time": "2021-12-24T18:57:59.945482",
     "exception": false,
     "start_time": "2021-12-24T18:57:59.779385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define the dataset function\n",
    "Below is our PyTorch dataset function. It always outputs tokens and attention. During training it also provides labels. And during inference it also provides word ids to help convert token predictions into word predictions.\n",
    "\n",
    "Note that we use `text.split()` and `is_split_into_words=True` when we convert train text to labeled train tokens. This is how the HugglingFace tutorial does it. However, this removes characters like `\\n` new paragraph. If you want your model to see new paragraphs, then we need to map words to tokens ourselves using `return_offsets_mapping=True`. See my TensorFlow notebook [here][1] for an example.\n",
    "\n",
    "Some of the following code comes from the example at HuggingFace [here][2]. However I think the code at that link is wrong. The HuggingFace original code is [here][3]. With the flag `LABEL_ALL` we can either label just the first subword token (when one word has more than one subword token). Or we can label all the subword tokens (with the word's label). In this notebook version, we label all the tokens. There is a Kaggle discussion [here][4]\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n",
    "[2]: https://huggingface.co/docs/transformers/custom_datasets#tok_ner\n",
    "[3]: https://github.com/huggingface/transformers/blob/86b40073e9aee6959c8c85fcba89e47b432c4f4d/examples/pytorch/token-classification/run_ner.py#L371\n",
    "[4]: https://www.kaggle.com/c/feedback-prize-2021/discussion/296713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "26ec4e93-6f56-4146-95e9-46b9903a2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  re\n",
    "LABEL_ALL_SUBTOKENS = True\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, get_wids):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.get_wids = get_wids # for validation\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # GET TEXT AND WORD LABELS\n",
    "        text = self.data.text[index]\n",
    "        word_labels = self.data.entities[index] if not self.get_wids else None\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s{2,}', '. ', text)\n",
    "        text = re.sub(r'\\xa0', ' ', text)\n",
    "        text = re.sub(r'\\.{6}','',text)\n",
    "        text = re.sub(r'\\.\\.', '.', text)\n",
    "        text = re.sub(r'\\. ', '. [SEP] [CLS] ' ,text)\n",
    "        text = re.sub(r'! ', '! [SEP] [CLS] ' ,text)\n",
    "        text = re.sub(r'\\? ', '? [SEP] [CLS] ' ,text)\n",
    "        # text = text[:-7]\n",
    "        # text= \"[CLS] \" + text\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(text.split(),\n",
    "                             is_split_into_words=True,\n",
    "                             add_special_tokens= True,\n",
    "                             #return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        word_ids = encoding.word_ids()  \n",
    "        \n",
    "        # CREATE TARGETS\n",
    "        if not self.get_wids:\n",
    "            previous_word_idx = None\n",
    "            previous_cls_ids = 0\n",
    "            sen_count = 0\n",
    "            label_ids = []\n",
    "            sen_label_ids = []\n",
    "            input_ids = encoding['input_ids']\n",
    "            for i, word_idx in enumerate(word_ids):                            \n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif input_ids[i] == 66: # SEP\n",
    "                    label_ids.append(-100)\n",
    "                    sen_count += 1\n",
    "                    if len(sen_label_ids) != 0:\n",
    "                        label_ids[previous_cls_ids] = max(sen_label_ids, key=sen_label_ids.count)\n",
    "                    else:\n",
    "                        label_ids[previous_cls_ids] = -100\n",
    "                elif input_ids[i] == 65: # CLS\n",
    "                    previous_cls_ids = i\n",
    "                    label_ids.append(-1) # wait for filling\n",
    "                    sen_label_ids = []\n",
    "                    sen_count += 1\n",
    "                elif word_idx != previous_word_idx:              \n",
    "                    label_ids.append( labels_to_ids[word_labels[word_idx-sen_count]] )\n",
    "                    sen_label_ids.append(label_ids[-1])\n",
    "                else:\n",
    "                    if LABEL_ALL_SUBTOKENS:\n",
    "                        label_ids.append( labels_to_ids[word_labels[word_idx-sen_count]] )\n",
    "                        sen_label_ids.append(label_ids[-1])\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            if len(sen_label_ids) != 0:\n",
    "                label_ids[previous_cls_ids]= max(sen_label_ids, key=sen_label_ids.count)\n",
    "            else:\n",
    "                label_ids[previous_cls_ids] = -100\n",
    "            encoding['labels'] = label_ids\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        if self.get_wids: \n",
    "            word_ids2 = [w if w is not None else -1 for w in word_ids]\n",
    "            item['wids'] = torch.as_tensor(word_ids2)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95239219",
   "metadata": {
    "papermill": {
     "duration": 0.166423,
     "end_time": "2021-12-24T18:58:00.631003",
     "exception": false,
     "start_time": "2021-12-24T18:58:00.464580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Train and Validation Dataloaders\n",
    "We will use the same train and validation subsets as my TensorFlow notebook [here][1]. Then we can compare results. And/or experiment with ensembling the validation fold predictions.\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51ea953f",
   "metadata": {
    "papermill": {
     "duration": 0.19708,
     "end_time": "2021-12-24T18:58:00.995662",
     "exception": false,
     "start_time": "2021-12-24T18:58:00.798582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts. We will split 90% 10% for validation.\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE VALIDATION INDEXES (that match my TF notebook)\n",
    "IDS = train_df.id.unique()\n",
    "print('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n",
    "\n",
    "# TRAIN VALID SPLIT 90% 10%\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "np.random.seed(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "\"[CLS] Phones and Driving, should it be a thing? [SEP] [CLS] Today over 5 billion people own a mobile device, and a little over 1 billion own some sort of vehicle. [SEP] [CLS] These are two of the things people feel they need in life. [SEP] [CLS] But maybe the use of both at once is a bit too overwhelming. [SEP] [CLS] Driving while texting, or even making a phone call is excessive. [SEP] [CLS] It can be dangerous and hurt people in multiple ways. [SEP] [CLS] Phone usage while driving should be prohibited in every state, country and continent!\\n\\nUsing a phone while driving a vehicle should not be allowed anywhere! [SEP] [CLS] It can cause many accidents. [SEP] [CLS] It can also bring troubles and pain to ones family. [SEP] [CLS] For example, in February 2018, a 17 year old in Alabaster, Alabama killed a ten year old and her father in an accident because the teen was texting while driving. [SEP] [CLS] If the teen would have been paying attention to the road and not their phone the accident maybe could have been prevented. [SEP] [CLS] Not saying all accidents are caused by phone use, but a majority of them are. [SEP] [CLS] Generally there are 1.6 million crashes involving texting and driving. [SEP] [CLS] Nearly 390,000 injuries occur each year from accidents involving texting and driving. [SEP] [CLS] 4,637 people died in the year 2018 because of the accidents. [SEP] [CLS] If that does not scare one into not using their phone while driving, then I don't know what will. [SEP] [CLS] This is one of the many reasons why I believe driving and the use of a phone should be banned!\\n\\nAnother reason cell phone use while driving should be banned; if adults use their phone while operating a vehicle their children will possibly be strongly influenced by it. [SEP] [CLS] The children will start to think it is okay to do so. [SEP] [CLS] It is the year 2019 and all that this generation raves about is having a car and the newest IPhone if we are taught that it is okay to text and drive, someone can earnestly get hurt by these actions. [SEP] [CLS] Most accidents involve teen driving, we don't want to give them another reason to be absentminded while driving. [SEP] [CLS] The use of a mobile device while operating a vehicle should be prohibited. [SEP] [CLS] Now is the time to admit that driving while using a phone or any mobile devices is dangerous! [SEP] [CLS] You wouldn't want you or a loved one to end up critically injured, dead or disabled. [SEP] [CLS] Phone usage while driving is one of the many reasons accidents happen everyday. [SEP] [CLS] You can always wait until you are out of the car to make that call or answer that text ! [SEP]\""
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = train_text_df.loc[train_idx[13],'text']\n",
    "text = text.strip()\n",
    "text = re.sub(r'\\.\\s{2,}', '. ', text)\n",
    "text = re.sub(r'\\.{6}','',text)\n",
    "text = re.sub(r'\\. ', '. [SEP] [CLS] ' ,text)\n",
    "text = re.sub(r'! ', '! [SEP] [CLS] ' ,text)\n",
    "text = re.sub(r'\\? ', '? [SEP] [CLS] ' ,text)\n",
    "# text = text[:-7]\n",
    "text= \"[CLS] \" + text +\" [SEP]\"\n",
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "03e635ad",
   "metadata": {
    "papermill": {
     "duration": 0.420181,
     "end_time": "2021-12-24T18:58:01.582956",
     "exception": false,
     "start_time": "2021-12-24T18:58:01.162775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15594, 3)\n",
      "TRAIN Dataset: (14034, 2)\n",
      "TEST Dataset: (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "# CREATE TRAIN SUBSET AND VALID SUBSET\n",
    "data = train_text_df[['id','text', 'entities']]\n",
    "train_dataset = data.loc[data['id'].isin(IDS[train_idx]),['text', 'entities']].reset_index(drop=True)\n",
    "test_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH) \n",
    "training_set = dataset(train_dataset, tokenizer, config['max_length'], False)\n",
    "testing_set = dataset(test_dataset, tokenizer, config['max_length'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "71ffa08c",
   "metadata": {
    "papermill": {
     "duration": 0.213192,
     "end_time": "2021-12-24T18:58:02.079798",
     "exception": false,
     "start_time": "2021-12-24T18:58:01.866606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAIN DATASET AND VALID DATASET\n",
    "train_params = {'batch_size': config['train_batch_size'],\n",
    "                'shuffle': True,\n",
    "                # 'num_workers': 2,\n",
    "                'num_workers': 0,\n",
    "                'pin_memory':True\n",
    "               }\n",
    "\n",
    "test_params = {'batch_size': config['valid_batch_size'],\n",
    "               'shuffle': False,\n",
    "               # 'num_workers': 2,\n",
    "               'num_workers': 0,\n",
    "               'pin_memory':True\n",
    "              }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "# TEST DATASET\n",
    "test_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\n",
    "test_texts_loader = DataLoader(test_texts_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "\"[CLS] Every day millions of children come home on a bus instead of staying after school, and for many, there is a very good reason. [SEP] [CLS] It seems to make sense, having everyone take a extra curricular activity, but in reality it doesn't work. [SEP] [CLS] Every student has a reason to go, or not to go to an extra curricular activity and some reasons are not just because of preference. [SEP] [CLS] Not everyone has a lifestyle that allows them to stay after school or do a sport, many of the students won't have a way to get home. [SEP] [CLS] The school will not be able to provide enough buses for these students either. [SEP] [CLS] There isn't even enough room in the current activity rooms to hold 78 kids, and the materials needed are to expensive. [SEP] [CLS] Having mandatory extra curricular activities will be a disaster because there is not enough room, it is to expensive, and students won't be able to get home. [SEP] [CLS] If our school system made extra curricular activities mandatory the first problem would be the amount of space needed to support the students activity. [SEP] [CLS] For example, there are almost 900 students in my school, This would mean in the 10 after school activities my school provides, there would need to be 90 kids in each one; That's enough to fill 4 classrooms. [SEP] [CLS] This brings up another problem, the school can't have 150 kids in one program and 10 in the other, so the school would have to chose the activity for us. [SEP] [CLS] Just that fact alone takes all the excitement out of the activity, just for reference, imagine the star quarter back on the football ending up in art instead, possibly destroying his or her whole football career. [SEP] [CLS] Most of my school's after school activities need certain spaces for the participants to practice and do their activity. [SEP] [CLS] For instance the robotics team only has one room with all of the vices, saws, and tools while the other robotics rooms will have students constantly running between the other rooms getting tools that they dont have in their completely different room. [SEP] [CLS] The cheer leading team will have to move to the gym to fit all the people, but so did the pep band, and basketball practice is already in the gym, but they could just go in the auxiliary gym which is smaller than the cafeteria, the problem with that is the wrestling team is already in the auxiliary gym. [SEP] [CLS] The school has no room to fit every team and all of the clubs. [SEP] [CLS] If Prince William County decided to make after school activities mandatory the schools would not be able to afford the materials needed for the programs. [SEP] [CLS] A dungeons and Dragons game along with all of the books and die is over 200 dollars, now imagining multiplying that by 7 just to afford the basics for the club. [SEP] [CLS] The robotics team pays around 600 dollars for each robot, and more for the competitions, now that program has to buy 15 robots and buy more slots at the competitions. [SEP] [CLS] If some how the schools were able to afford these expensive along with all other clubs, teachers, and staff salaries it would come from our local taxes. [SEP] [CLS] This would double our local taxes, but if the schools decide not to over tax the citizens of Generic_School than there would be no way the school could provide the gear and equipment for the school's clubs and teams. [SEP] [CLS] My school's sports gear is already worn out and our schools are in dire need of new equipment, but there is no way we could spend 7 times as much just to give everyone more cheap and basic equipment. [SEP] [CLS] while most kids in the school do not participate in a extra curricular activities because they don't want to, many students don't participate because they can't. [SEP] [CLS] On most days my dad gets home at around six so when my activity ends at 4:30 in the afternoon i have to take the bus. [SEP] [CLS] Currently my school has 2 activity buses, one for Generic_City and one for Generic_City. [SEP] [CLS] The activity bus arrives at my home which is 5 miles away at 5:40 in the evening, it takes the bus 1 hour and 10 minutes to drop me off at my home. [SEP] [CLS] With a direct route the bus would have to travel at 4 mph. [SEP] [CLS] There is no way the school can hire enough buses to work almost 12 straight hours a day. [SEP] [CLS] If the school really can't find enough buses each bus would have to make 70 stops. [SEP] [CLS] Many students won't be able to be picked up by their parents and will be forced to endure the 3 hour bus ride through a city with only 30 square miles of residential area. [SEP] [CLS] If our schools really made after school activities mandatory, it would be a disaster. [SEP] [CLS] One of the biggest issues with this topic is that in its simple form it looks like a great idea, but when you look at the details and what it would cause it shows the big mistake it would be. [SEP] [CLS] Giving every student the chance to do an interactive activity they enjoy would be incredible. [SEP] [CLS] Sadly, doing that in our current school system would cause all after school activities to stop going to competitions due to the over crowding; The students would not be able to pick their own activity and would be forced to do something they are bad at or just dislike. [SEP] [CLS] Making the activities would not only be a punishment to the students of Generic_School but to the teachers, the bus drivers, the custodians, the administrators, the taxpayers, and even our entire county's well being. [SEP] [CLS] In this scenario it may at first look like everybody wins, but really when you look at the real facts everybody loses. [SEP] [CLS] The entire mentality of school will become even more negative as students are stripped of their choice to do something they enjoy and taken away from time with their family. [SEP] [CLS] Although the goal of this rule would be to create a good learning environment and a chance to participate in more interactive learning experiences,our county administrators might fail to see the real outcome of this decision. [SEP] [CLS] In conclusion, enforcing after school activities will take away the last form of student's choice in our schools. [SEP]\""
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train_dataset.loc[692,'text']\n",
    "text = text.strip()\n",
    "text = re.sub(r'\\s{2,}', '. ', text)\n",
    "text = re.sub(r'\\xa0', ' ', text)\n",
    "text = re.sub(r'\\.{6}','',text)\n",
    "text = re.sub(r'\\.\\.', '.', text)\n",
    "text = re.sub(r'\\. ', '. [SEP] [CLS] ' ,text)\n",
    "text = re.sub(r'! ', '! [SEP] [CLS] ' ,text)\n",
    "text = re.sub(r'\\? ', '? [SEP] [CLS] ' ,text)\n",
    "# text = text[:-7]\n",
    "text= \"[CLS] \" + text +\" [SEP]\"\n",
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "\"[CLS] Fellow Citizens, there are many reasons why limiting car usgage has outstanding oppritunities and advantages.[SEP][CLS] For an example, If there aren't people out there spending their hard earned money that they worked for, for an expensive car that they are just going to use to get to someplace faster when they could just stop being lazy for once and walk or run, ride bikes, hike, or take a bus, instead of spending money after money to fill a car up to just waste gas, and to fill it up again.[SEP][CLS] Running, riding bikes, hiking, etc.[SEP][CLS] gives you a chance to experience reality.[SEP][CLS] Being outside gives people a chance to see the earths real beauty, and to smell the fresh air.[SEP][CLS] Limiting car usage helps people stay in shape and keeps them healthy if their not just sitting in a car for half a hour when they could be running or walking.[SEP][CLS] Residents in Vauban, Germany are permitted car ownership, but there are only two places to park : large garages at the edge of the development, where a car-owner buys a space, for $40,000, along with a home.[SEP][CLS] About 70 percent of Vauban's families do not iwn cars, an about 57 percent sold a car to move there.[SEP][CLS] Having a car makes peoples tense, and being in a place where there isnt alot of car usgae makes people much happier.[SEP][CLS] People who live in places where cars arent used as oftn as other states, those towns place stores in walking distance, on a main street, rather than in malls along some distant highway.[SEP][CLS] An amazing advantage that people can can from limiting the usage of cars is less drunk drivers, or even none.[SEP][CLS] If there's less drunk drivers, there's less car crashes and less deaths.[SEP][CLS] Everyone understands that you have to be 21 or older in most states to drink alcoholic beverages.[SEP][CLS] The big adavntage out of this is, if older people were to get drunk, they would be forced to call a cab, or even walk home.[SEP][CLS] Their not putting noone else's life in danger, and this would lower the crashes and death fatalities per year.[SEP][CLS] As as many cars that there is in the world, all the use of them would probably pollute the world.[SEP][CLS] Paris enforce a partial driving ban to clear the air of the global city after days of the near-record pollution.[SEP][CLS] Limiting car usage gives you the oppritunity to be with your family and friends and spend more time with them then you normally would having a car.[SEP][CLS] You can walk and talk with them on the way to work, you can stop by the park and have some fun on your way to the grocery store.[SEP][CLS] The best part about this is, you dont have to worry about the roads being busy.[SEP][CLS] You ca walk, run, hike, and bike without having to hear the noisy streets, the honking, the screeching, etc.[SEP][CLS] What's the point in having a car that your just going to be wasting money on, when running, walking or riding a bike is free and even cheaper and safer than driving a car.[SEP][CLS] What are you gonna do when your car breaks down?[SEP][CLS] Are you gonna keep spending your money on when it keeps breaking?[SEP][CLS] Why do that when you have two perfectly good and healthy legs and you can run or walk somewhere?.[SEP][CLS] Just take a moment to think about the fines every single person breathing in the world has received?[SEP][CLS] Alot of money wasted on something idiotic huh?[SEP][CLS] just imagine if every state, continents, islands, and places we dont know about all stopped using cars, or even just limited the car usage.[SEP][CLS] We'd be saving a whole ton of money, we'd basically all be middle classes.[SEP][CLS] This whole situation is a win-win, You get to be with your family 24/7, Running, walking, biking, hiking etc.[SEP][CLS] is all an amazing way to keep people healthy.[SEP][CLS] People dont have to worry about drunk drivers, or car crashes, or car fatalities.[SEP][CLS] People can finally stop spending their hard earned money that they're just going keep spending and wasting on gasoline or to fix your car when its having problems and it breaks.[SEP][CLS] Families can save the money that they would normally be spending on gasoline and start saving for a trip to take their family on to spend more time with them, or to take a trip for yourself.[SEP][CLS] There are plently more reasons and outstanding advantages that limiting car usage gives you, but these are some of the very best![SEP][CLS] Just remember, The only good advantages a car can give you is getting you somewhere faster, and keeping you comfortable and safe.[SEP][CLS] But they are also the most danagerous, and expensive things out there.[SEP][CLS] They can take your life or someone elses[SEP]\""
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(training_set[20]['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Every day millions of children come home on a bus instead of staying after school, and for many, there is a very good reason. It seems to make sense, having everyone take a extra curricular activity, but in reality it doesn't work. Every student has a reason to go, or not to go to an extra curricular activity and some reasons are not just because of preference. Not everyone has a lifestyle that allows them to stay after school or do a sport, many of the students won't have a way to get home. The school will not be able to provide enough buses for these students either. There isn't even enough room in the current activity rooms to hold 78 kids, and the materials needed are to expensive. Having mandatory extra curricular\\xa0activities will be a disaster because there is not enough room, it is to expensive, and students won't be able to get home.\\n\\nIf our school system made extra curricular activities mandatory the first problem would be the amount of space needed to support the students activity. For example, there are almost 900 students in my school, This would mean in the 10 after school activities my school provides, there would need to be 90 kids in each one; That's enough to fill 4 classrooms. This brings up another problem, the school can't have 150 kids in one program and 10 in the other, so the school would have to chose the activity for us. Just that fact alone takes all the excitement out of the activity, just for reference, imagine the star quarter back on the football ending up in art instead, possibly destroying his or her whole football career. Most of my school's after school\\xa0activities need certain spaces for the participants to practice and do their activity. For instance the robotics team only has one room with all of the vices, saws, and tools while the other robotics rooms will have students constantly running between the other rooms getting tools that they dont have in their completely different room. The cheer leading\\xa0team will have to move to the gym to fit all the people, but so did the pep band, and basketball practice is already in the gym, but they could just go in the auxiliary gym which is smaller than the cafeteria, the problem with that is the wrestling team is already in the auxiliary gym. The school has no room to fit every team and all of the clubs.\\n\\nIf Prince William\\xa0County decided to make after school\\xa0activities mandatory the schools would not be able to afford the materials needed for the programs. A dungeons and Dragons game along with all of the books and die is over 200 dollars, now imagining multiplying that by 7 just to afford the basics for the club. The robotics team pays around 600 dollars for each robot, and more for the competitions, now that program has to buy 15 robots and buy more slots at the competitions. If some how the schools were able to afford these expensive along with all other clubs, teachers, and staff salaries it would come from our local taxes. This would double our local taxes, but if the schools decide not to over tax the citizens of Generic_School than there would be no way the school could provide the gear and equipment for the school's clubs and teams. My school's sports gear is already worn out and our schools are in dire need of new equipment, but there is no way we could spend 7 times as much just to give everyone more cheap and basic equipment.\\n\\nwhile most kids in the school do not participate in a extra curricular activities because they don't want to, many students don't participate because they can't. On most days my dad gets home at around six so when my activity ends at 4:30 in the afternoon i have to take the bus. Currently my school has 2 activity buses, one for Generic_City and one for Generic_City. The activity bus arrives at my home which is 5 miles away at 5:40 in the evening, it takes the bus 1 hour and 10 minutes to drop me off at my home. With a direct route the bus would have to travel at 4 mph. There is no way the school can hire enough buses to work almost 12 straight hours a day. If the school really can't find enough buses each bus would have to make 70 stops. Many students won't be able to be picked up by their parents and will be forced to endure the 3 hour bus ride through a city with only 30 square miles of residential area.\\n\\nIf our schools really made after school activities mandatory, it would be a disaster. One of the biggest issues with this topic is that in its simple form it looks like a great idea, but when you look at the details and what it would cause it shows the big mistake it would be. Giving every student the chance to do an interactive activity they enjoy would be incredible. Sadly, doing that in our current school system would cause all after school activities to stop going to competitions due to the over crowding; The students would not be able to pick their own activity and would be forced to do something they are bad at or just dislike. Making the activities would not only be a punishment to the students of Generic_School but to the teachers, the bus drivers, the custodians, the administrators, the taxpayers, and even our entire county's well being. In this scenario it may at first look like everybody wins, but really when you look at the real facts everybody loses. The entire mentality of school will become even more negative as students are stripped of their choice to do something they enjoy and taken away from time with their family. Although the goal of this rule would be to create a good learning environment and a chance to participate in more interactive learning experiences,our county administrators might fail to see the real outcome of this decision. In conclusion, enforcing after school\\xa0activities will take away the last form of student's choice in our schools.\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\""
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.loc[692,'text']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# for i in range(len(train_idx)):\n",
    "#     try:\n",
    "#         res = sum(training_set[i]['labels']==-1)\n",
    "#         if res!=0:\n",
    "#             print('-1 label')\n",
    "#             print(i)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(i)\n",
    "\n",
    "# training_set[14]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fe5d0be5-bad2-4c48-8f96-99a41f49c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(14034):\n",
    "#     try:\n",
    "#         x = training_set[i]\n",
    "#     except:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6193f5c-c413-41d9-927a-a290451a62f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"[CLS] Every day millions of children come home on a bus instead of staying after school, and for many, there is a very good reason.[SEP][CLS] It seems to make sense, having everyone take a extra curricular activity, but in reality it doesn't work.[SEP][CLS] Every student has a reason to go, or not to go to an extra curricular activity and some reasons are not just because of preference.[SEP][CLS] Not everyone has a lifestyle that allows them to stay after school or do a sport, many of the students won't have a way to get home.[SEP][CLS] The school will not be able to provide enough buses for these students either.[SEP][CLS] There isn't even enough room in the current activity rooms to hold 78 kids, and the materials needed are to expensive.[SEP][CLS] Having mandatory extra curricular activities will be a disaster because there is not enough room, it is to expensive, and students won't be able to get home.[SEP][CLS] If our school system made extra curricular activities mandatory the first problem would be the amount of space needed to support the students activity.[SEP][CLS] For example, there are almost 900 students in my school, This would mean in the 10 after school activities my school provides, there would need to be 90 kids in each one; That's enough to fill 4 classrooms.[SEP][CLS] This brings up another problem, the school can't have 150 kids in one program and 10 in the other, so the school would have to chose the activity for us.[SEP][CLS] Just that fact alone takes all the excitement out of the activity, just for reference, imagine the star quarter back on the football ending up in art instead, possibly destroying his or her whole football career.[SEP][CLS] Most of my school's after school activities need certain spaces for the participants to practice and do their activity.[SEP][CLS] For instance the robotics team only has one room with all of the vices, saws, and tools while the other robotics rooms will have students constantly running between the other rooms getting tools that they dont have in their completely different room.[SEP][CLS] The cheer leading team will have to move to the gym to fit all the people, but so did the pep band, and basketball practice is already in the gym, but they could just go in the auxiliary gym which is smaller than the cafeteria, the problem with that is the wrestling team is already in the auxiliary gym.[SEP][CLS] The school has no room to fit every team and all of the clubs.[SEP][CLS] If Prince William County decided to make after school activities mandatory the schools would not be able to afford the materials needed for the programs.[SEP][CLS] A dungeons and Dragons game along with all of the books and die is over 200 dollars, now imagining multiplying that by 7 just to afford the basics for the club.[SEP][CLS] The robotics team pays around 600 dollars for each robot, and more for the competitions, now that program has to buy 15 robots and buy more slots at the competitions.[SEP][CLS] If some how the schools were able to afford these expensive along with all other clubs, teachers, and staff salaries it would come from our local taxes.[SEP][CLS] This would double our local taxes, but if the schools decide not to over tax the citizens of Generic_School than there would be no way the school could provide the gear and equipment for the school's clubs and teams.[SEP][CLS] My school's sports gear is already worn out and our schools are in dire need of new equipment, but there is no way we could spend 7 times as much just to give everyone more cheap and basic equipment.[SEP][CLS] while most kids in the school do not participate in a extra curricular activities because they don't want to, many students don't participate because they can't.[SEP][CLS] On most days my dad gets home at around six so when my activity ends at 4:30 in the afternoon i have to take the bus.[SEP][CLS] Currently my school has 2 activity buses, one for Generic_City and one for Generic_City.[SEP][CLS] The activity bus arrives at my home which is 5 miles away at 5:40 in the evening, it takes the bus 1 hour and 10 minutes to drop me off at my home.[SEP][CLS] With a direct route the bus would have to travel at 4 mph.[SEP][CLS] There is no way the school can hire enough buses to work almost 12 straight hours a day.[SEP][CLS] If the school really can't find enough buses each bus would have to make 70 stops.[SEP][CLS] Many students won't be able to be picked up by their parents and will be forced to endure the 3 hour bus ride through a city with only 30 square miles of residential area.[SEP][CLS] If our schools really made after school activities mandatory, it would be a disaster.[SEP][CLS] One of the biggest issues with this topic is that in its simple form it looks like a great idea, but when you look at the details and what it would cause it shows the big mistake it would be.[SEP][CLS] Giving every student the chance to do an interactive activity they enjoy would be incredible.[SEP][CLS][SEP]\""
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(training_set[692]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "165d0927-ba3d-41d7-a510-2a3e9ea08b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4)"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]['labels'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "896b4e97-7367-4512-9008-8be61de445e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([   4,    3,    3,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n           4,    4,    4, -100,    4,    4,    4,    4,    4,    4,    4,    4,\n           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n           4,    4,    4,    4, -100,    4,    4,    4,    4,    4,    4,    4,\n           4,    4,    4,    4, -100,    6,    5,    6,    6,    6,    6,    6,\n           6,    6,    6,    6,    6, -100,   12,   11,   12,   12,   12,   12,\n          12,   12,   12,   12,   12,   12,   12,   12,   12,   12,   12,   12,\n          12,   12,   12,   12,   12,   12,   12,   12,   12,   12, -100,    6,\n           0,    0,    0,    0,    5,    6,    6,    6,    6,    6,    6,    6,\n           6,    6,    6, -100,   12,   11,   12,   12,   12,   12,   12, -100,\n          12,   12,   12,   12,   12,   12,   12,   12,   12,   12,   12,   12,\n          12,   12,   12,   12,   12,   12,   12,   12,   12,   12,   12,   12,\n        -100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0, -100,    8,    7,    8,\n           8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,\n           8,    8,    8,    8,    8,    8,    8,    8,    8, -100,   10,    9,\n          10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,\n          10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,\n          10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,\n          10,   10,   10,   10,   10,   10,   10,   10,   10, -100,    8,    7,\n           8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,\n           8,    8,    8, -100,   10,    9,   10,   10,   10,   10,   10,   10,\n          10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,\n          10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,\n          10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,\n          10,   10,   10,   10,   10,   10,   10, -100,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0, -100,   14,   13,   14,   14,   14,   14,   14,   14,\n          14,   14, -100,   14,   14,   14,   14,   14,   14,   14,   14,   14,\n          14,   14,   14,   14,   14,   14,   14,   14,   14,   14,   14,   14,\n          14,   14, -100,   14,   14,   14,   14,   14,   14,   14,   14,   14,\n          14,   14,   14, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100])"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]['labels'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "23a40486-35de-4e05-8b8a-69ac83504e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f31ae79",
   "metadata": {
    "papermill": {
     "duration": 0.177586,
     "end_time": "2021-12-24T18:58:02.426035",
     "exception": false,
     "start_time": "2021-12-24T18:58:02.248449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model\n",
    "The PyTorch train function is taken from Raghavendrakotala's great notebook [here][1]. I assume it uses a masked loss which avoids computing loss when target is `-100`. If not, we need to update this.\n",
    "\n",
    "In Kaggle notebooks, we will train our model for 5 epochs `batch_size=4` with Adam optimizer and learning rates `LR = [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7]`. The loaded model was trained offline with `batch_size=8` and `LR = [5e-5, 5e-5, 5e-6, 5e-6, 5e-7]`. (Note the learning rate changes `e-5`, `e-6`, and `e-7`). Using `batch_size=4` will probably achieve a better validation score than `batch_size=8`, but I haven't tried yet.\n",
    "\n",
    "[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d0bb7759",
   "metadata": {
    "papermill": {
     "duration": 0.18811,
     "end_time": "2021-12-24T18:58:02.784655",
     "exception": false,
     "start_time": "2021-12-24T18:58:02.596545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    #tr_preds, tr_labels = [], []\n",
    "    \n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(training_loader, total=len(training_loader))):\n",
    "        \n",
    "        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n",
    "        labels = batch['labels'].to(config['device'], dtype = torch.long)\n",
    "\n",
    "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n",
    "                               return_dict=False)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 1000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        #tr_labels.extend(labels)\n",
    "        #tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=config['max_grad_norm']\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a78174ac",
   "metadata": {
    "papermill": {
     "duration": 12.035834,
     "end_time": "2021-12-24T18:58:14.988658",
     "exception": false,
     "start_time": "2021-12-24T18:58:02.952824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE MODEL\n",
    "config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                   DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\n",
    "model.to(config['device'])\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37429c8d-8115-4c48-9988-62c074684912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "856da230-7021-4bd0-a733-ac9cde2b19e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d321f2ae-c02b-43cd-b7e1-b54e9dae0aee",
   "metadata": {
    "papermill": {
     "duration": 6.881216,
     "end_time": "2021-12-24T18:58:22.040114",
     "exception": false,
     "start_time": "2021-12-24T18:58:15.158898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training epoch: 1\n",
      "### LR = 2.5e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14034 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 0000 training steps: 2.6564600467681885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 183/14034 [01:13<1:32:30,  2.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_14268/1506785656.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'### LR = {lr}\\n'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m         \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty_cache\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0mgc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_14268/3610671926.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(epoch)\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[1;31m# backward pass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 52\u001B[1;33m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     53\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Programm\\Python\\Anaconda\\envs\\mlp\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    306\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 307\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    308\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    309\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Programm\\Python\\Anaconda\\envs\\mlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    154\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[0;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 156\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001B[0m\u001B[0;32m    157\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "## ignore __floordiv__ is deprecated\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# LOOP TO TRAIN MODEL (or load model)\n",
    "if not LOAD_MODEL_FROM:\n",
    "    for epoch in range(config['epochs']):\n",
    "        \n",
    "        print(f\"### Training epoch: {epoch + 1}\")\n",
    "        for g in optimizer.param_groups: \n",
    "            g['lr'] = config['learning_rates'][epoch]\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'### LR = {lr}\\n')\n",
    "        \n",
    "        train(epoch)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    torch.save(model.state_dict(), f'bigbird_v{TRAIN_VER}.pt')\n",
    "else:\n",
    "    model.load_state_dict(torch.load(f'{LOAD_MODEL_FROM}/bigbird_v{LOAD_VER}.pt'))\n",
    "    print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "0a16b77d",
   "metadata": {
    "papermill": {
     "duration": 0.172006,
     "end_time": "2021-12-24T18:58:22.383000",
     "exception": false,
     "start_time": "2021-12-24T18:58:22.210994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference and Validation Code\n",
    "We will infer in batches using our data loader which is faster than inferring one text at a time with a for-loop. The metric code is taken from Rob Mulla's great notebook [here][2]. Our model achieves validation F1 score 0.615! \n",
    "\n",
    "During inference our model will make predictions for each subword token. Some single words consist of multiple subword tokens. In the code below, we use a word's first subword token prediction as the label for the entire word. We can try other approaches, like averaging all subword predictions or taking `B` labels before `I` labels etc.\n",
    "\n",
    "[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "[2]: https://www.kaggle.com/robikscube/student-writing-competition-twitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f6c46",
   "metadata": {
    "papermill": {
     "duration": 0.183064,
     "end_time": "2021-12-24T18:58:22.738149",
     "exception": false,
     "start_time": "2021-12-24T18:58:22.555085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(batch):\n",
    "                \n",
    "    # MOVE BATCH TO GPU AND INFER\n",
    "    ids = batch[\"input_ids\"].to(config['device'])\n",
    "    mask = batch[\"attention_mask\"].to(config['device'])\n",
    "    outputs = model(ids, attention_mask=mask, return_dict=False)\n",
    "    all_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy() \n",
    "\n",
    "    # INTERATE THROUGH EACH TEXT AND GET PRED\n",
    "    predictions = []\n",
    "    for k,text_preds in enumerate(all_preds):\n",
    "        token_preds = [ids_to_labels[i] for i in text_preds]\n",
    "\n",
    "        prediction = []\n",
    "        word_ids = batch['wids'][k].numpy()  \n",
    "        previous_word_idx = -1\n",
    "        for idx,word_idx in enumerate(word_ids):                            \n",
    "            if word_idx == -1:\n",
    "                pass\n",
    "            elif word_idx != previous_word_idx:              \n",
    "                prediction.append(token_preds[idx])\n",
    "                previous_word_idx = word_idx\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3077d",
   "metadata": {
    "papermill": {
     "duration": 0.18449,
     "end_time": "2021-12-24T18:58:23.115470",
     "exception": false,
     "start_time": "2021-12-24T18:58:22.930980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/zzy990106/pytorch-ner-infer\n",
    "# code has been modified from original\n",
    "def get_predictions(df=test_dataset, loader=testing_loader):\n",
    "    \n",
    "    # put model in training mode\n",
    "    model.eval()\n",
    "    \n",
    "    # GET WORD LABEL PREDICTIONS\n",
    "    y_pred2 = []\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        labels = inference(batch)\n",
    "        y_pred2.extend(labels)\n",
    "\n",
    "    final_preds2 = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "\n",
    "        idx = df.id.values[i]\n",
    "        #pred = [x.replace('B-','').replace('I-','') for x in y_pred2[i]]\n",
    "        pred = y_pred2[i] # Leave \"B\" and \"I\"\n",
    "        preds = []\n",
    "        j = 0\n",
    "        while j < len(pred):\n",
    "            cls = pred[j]\n",
    "            if cls == 'O': j += 1\n",
    "            else: cls = cls.replace('B','I') # spans start with B\n",
    "            end = j + 1\n",
    "            while end < len(pred) and pred[end] == cls:\n",
    "                end += 1\n",
    "            \n",
    "            if cls != 'O' and cls != '' and end - j > 7:\n",
    "                final_preds2.append((idx, cls.replace('I-',''),\n",
    "                                     ' '.join(map(str, list(range(j, end))))))\n",
    "        \n",
    "            j = end\n",
    "        \n",
    "    oof = pd.DataFrame(final_preds2)\n",
    "    oof.columns = ['id','class','predictionstring']\n",
    "\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83303a5",
   "metadata": {
    "papermill": {
     "duration": 0.318739,
     "end_time": "2021-12-24T18:58:23.602271",
     "exception": false,
     "start_time": "2021-12-24T18:58:23.283532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from Rob Mulla @robikscube\n",
    "# https://www.kaggle.com/robikscube/student-writing-competition-twitch\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97422c80",
   "metadata": {
    "papermill": {
     "duration": 104.195246,
     "end_time": "2021-12-24T19:00:08.079871",
     "exception": false,
     "start_time": "2021-12-24T18:58:23.884625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if COMPUTE_VAL_SCORE: # note this doesn't run during submit\n",
    "    # VALID TARGETS\n",
    "    valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n",
    "\n",
    "    # OOF PREDICTIONS\n",
    "    oof = get_predictions(test_dataset, testing_loader)\n",
    "\n",
    "    # COMPUTE F1 SCORE\n",
    "    f1s = []\n",
    "    CLASSES = oof['class'].unique()\n",
    "    print()\n",
    "    for c in tqdm(CLASSES):\n",
    "        pred_df = oof.loc[oof['class']==c].copy()\n",
    "        gt_df = valid.loc[valid['discourse_type']==c].copy()\n",
    "        f1 = score_feedback_comp(pred_df, gt_df)\n",
    "        print(c, f1)\n",
    "        f1s.append(f1)\n",
    "    print()\n",
    "    print('Overall', np.mean(f1s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c952e",
   "metadata": {
    "papermill": {
     "duration": 0.167874,
     "end_time": "2021-12-24T19:00:08.424099",
     "exception": false,
     "start_time": "2021-12-24T19:00:08.256225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer Test Data and Write Submission CSV\n",
    "We will now infer the test data and write submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1ed92",
   "metadata": {
    "papermill": {
     "duration": 0.759469,
     "end_time": "2021-12-24T19:00:09.351580",
     "exception": false,
     "start_time": "2021-12-24T19:00:08.592111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = get_predictions(test_texts, test_texts_loader)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dec595",
   "metadata": {
    "papermill": {
     "duration": 0.180565,
     "end_time": "2021-12-24T19:00:09.707700",
     "exception": false,
     "start_time": "2021-12-24T19:00:09.527135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed65a6-823f-443d-81f6-1d9adbc31bca",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886353c-2081-43ab-8086-28540b00d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d58d89-09d2-4280-87ef-3b9b5c9a68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "print(tokenizers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39de66f7-fc6e-47dc-872a-8ae90cdd3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd4d64-5461-4ae6-8bee-aa938a12d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1,2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe6f86-fce8-425c-98d8-8ad5d2af2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58225049-39de-4b16-b811-817e88540dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae476df-031c-47d0-a28d-0674c7ba21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3880c7a-a438-4a46-a26b-b6984bb5ca49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 228.514779,
   "end_time": "2021-12-24T19:00:12.608880",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-24T18:56:24.094101",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}